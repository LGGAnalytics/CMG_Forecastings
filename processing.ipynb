{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4af4c-358c-4633-a672-e8d28ea1eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pyathena import connect\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "\n",
    "# Your modules (ensure these are in the container/source_dir)\n",
    "from pooled_ridge import ForecastConfig, FeatureBuilder\n",
    "from runner_fm import sanitize_training_features, FEATURES  # adjust if module name differs\n",
    "\n",
    "from feature_store import *\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ---------------------------\n",
    "# Arg parsing\n",
    "# ---------------------------\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Athena config (matches how you're already querying)\n",
    "    parser.add_argument(\"--athena-database\", type=str, required=True)\n",
    "    parser.add_argument(\"--athena-table\", type=str, required=True)\n",
    "    parser.add_argument(\"--athena-work-group\", type=str, default=\"primary\")\n",
    "    parser.add_argument(\"--athena-staging-dir\", type=str, required=True)\n",
    "\n",
    "    # Extra inputs (Excel or others) mounted as ProcessingInputs\n",
    "    parser.add_argument(\"--calendar-path\", type=str, default=None)\n",
    "    parser.add_argument(\"--events-path\", type=str, default=None)\n",
    "    parser.add_argument(\"--cpi-path\", type=str, default=None)\n",
    "\n",
    "    # Output for training step\n",
    "    parser.add_argument(\"--output-features-path\", type=str,\n",
    "                        default=\"/opt/ml/processing/output\")\n",
    "\n",
    "    # Feature Store / AWS\n",
    "    parser.add_argument(\"--region\", type=str,\n",
    "                        default=os.getenv(\"AWS_REGION\", \"eu-west-1\"))\n",
    "    parser.add_argument(\"--feature-group-name\", type=str, required=True)\n",
    "    parser.add_argument(\"--offline-store-s3-uri\", type=str, required=True)\n",
    "    parser.add_argument(\"--role-arn\", type=str, required=True)\n",
    "    parser.add_argument(\"--enable-online-store\", action=\"store_true\")\n",
    "\n",
    "    # Required FS columns\n",
    "    parser.add_argument(\"--record-identifier-name\", type=str, default=\"record_id\")\n",
    "    parser.add_argument(\"--event-time-feature-name\", type=str, default=\"event_time\")\n",
    "\n",
    "    # Ingestion parallelism\n",
    "    parser.add_argument(\"--max-processes\", type=int, default=1)\n",
    "    parser.add_argument(\"--max-workers\", type=int, default=4)\n",
    "\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "# ---------------------------\n",
    "# IO helpers\n",
    "# ---------------------------\n",
    "def read_first_table_from_path(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV/Parquet/Excel from a directory or a file.\n",
    "\n",
    "    In your Pipeline, map ProcessingInput to these dirs:\n",
    "      - base  -> /opt/ml/processing/base\n",
    "      - calendar -> /opt/ml/processing/calendar\n",
    "      - etc.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        files = sorted(\n",
    "            f\n",
    "            for f in os.listdir(path)\n",
    "            if f.lower().endswith((\".csv\", \".parquet\", \".xlsx\", \".xls\"))\n",
    "        )\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No supported files found in: {path}\")\n",
    "        path = os.path.join(path, files[0])\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    if ext == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if ext in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "\n",
    "    raise ValueError(f\"Unsupported file type: {path}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Sessions\n",
    "    boto_session = boto3.Session(region_name=args.region)\n",
    "    sagemaker_session = Session(boto_session=boto_session)\n",
    "\n",
    "    logger.info(\n",
    "        \"Querying Athena table %s.%s via workgroup %s\",\n",
    "        args.athena_database,\n",
    "        args.athena_table,\n",
    "        args.athena_work_group,\n",
    "    )\n",
    "\n",
    "    conn = connect(\n",
    "        s3_staging_dir=args.athena_staging_dir,\n",
    "        region_name=args.region,\n",
    "        work_group=args.athena_work_group,\n",
    "    )\n",
    "\n",
    "    sql = f\"SELECT * FROM {args.athena_database}.{args.athena_table}\"\n",
    "    base_df = pd.read_sql(sql, conn)\n",
    "    logger.info(\"Loaded %d rows from Athena\", len(base_df))\n",
    "\n",
    "    # 2. Load Excel/other side inputs (from ProcessingInputs)\n",
    "    calendar_df = read_first_table_from_path(args.calendar_path)\n",
    "    events_df = read_first_table_from_path(args.events_path)\n",
    "    cpi_df = read_first_table_from_path(args.cpi_path)\n",
    "\n",
    "    if calendar_df.empty:\n",
    "        raise RuntimeError(\"Calendar data is required but missing/empty.\")\n",
    "\n",
    "\n",
    "    # 3. Build feature table with your existing logic\n",
    "    cfg = ForecastConfig(feature_list=FEATURES)\n",
    "    fb = FeatureBuilder(cfg)\n",
    "\n",
    "    feats = fb.build_feature_table(\n",
    "        base_df=base_df,\n",
    "        calendar_df=calendar_df,\n",
    "        events=events_df if not events_df.empty else None,\n",
    "        cpi=cpi_df if not cpi_df.empty else None,\n",
    "    )\n",
    "\n",
    "    # 4. Final sanitation before training\n",
    "    feats = sanitize_training_features(feats, FEATURES, cfg.target)\n",
    "\n",
    "    # 5. Add FS required columns (adapt to your config layout)\n",
    "    date_col = cfg.time_cols[0]\n",
    "    fy_col = cfg.time_cols[1]\n",
    "    fp_col = cfg.time_cols[2]\n",
    "\n",
    "    feats[date_col] = pd.to_datetime(feats[date_col], errors=\"coerce\")\n",
    "    feats[args.event_time_feature_name] = (\n",
    "        feats[date_col].astype(\"int64\") // 10**9\n",
    "    )\n",
    "\n",
    "    feats[args.record_identifier_name] = (\n",
    "        feats[cfg.store_col].astype(str).str.strip()\n",
    "        + \"_\"\n",
    "        + feats[fy_col].astype(int).astype(str).str.zfill(4)\n",
    "        + \"_\"\n",
    "        + feats[fp_col].astype(int).astype(str).str.zfill(2)\n",
    "    )\n",
    "\n",
    "    feats = feats.dropna(\n",
    "        subset=[args.record_identifier_name, args.event_time_feature_name]\n",
    "    )\n",
    "\n",
    "    logger.info(\"Final features: %d rows, %d columns\", *feats.shape)\n",
    "\n",
    "    # 6. Save for training step\n",
    "    os.makedirs(args.output_features_path, exist_ok=True)\n",
    "    out_path = os.path.join(args.output_features_path, \"training_features.parquet\")\n",
    "    feats.to_parquet(out_path, index=False)\n",
    "    logger.info(\"Wrote training features to %s\", out_path)\n",
    "\n",
    "    # 7. Ingest into Feature Store\n",
    "    fg = get_or_create_feature_group(\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        feature_group_name=args.feature_group_name,\n",
    "        df=feats,\n",
    "        record_identifier_name=args.record_identifier_name,\n",
    "        event_time_feature_name=args.event_time_feature_name,\n",
    "        offline_store_s3_uri=args.offline_store_s3_uri,\n",
    "        role_arn=args.role-arn if hasattr(args, \"role-arn\") else args.role_arn,\n",
    "        enable_online_store=args.enable_online_store,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Ingesting into Feature Group '%s'...\", args.feature_group_name)\n",
    "    fg.ingest(\n",
    "        data_frame=feats,\n",
    "        max_processes=args.max_processes,\n",
    "        max_workers=args.max_workers,\n",
    "        wait=True,\n",
    "    )\n",
    "    logger.info(\"Feature Store ingestion complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
